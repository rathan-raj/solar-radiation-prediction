# -*- coding: utf-8 -*-
"""SolarPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DRAs9GRe-bXaonDlA2CF6aIfDvMfjrqq

# **Prediction of Solar Radiation**

## *Big Data Analytics Life Cycle*

#### Stage1: Business Case Evaluation - The goal of this project is to predict the solar radiation level by using other independent features.

#### Stage2: Data Identification - We identified our dataset from one of the most popular dataset collections website Kaggle which is an external resource.

#### Stage3: Data Acquisition and Filtering
"""

# Commented out IPython magic to ensure Python compatibility.
#importing required libraries to deal with the dataset
import pandas as pd
import numpy as np
from datetime import datetime
# %matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt

#reading the csv file using pandas read_csv function
df = pd.read_csv('SolarPrediction.csv')

#getting the dimensions of the dataset
print('The number of rows and columns in this datset are:', df.shape)

#printing the columns present in the dataset
df.columns

#getting the datatypes of the columns
df.dtypes

#description of the data
df.describe()

#printing the head elements of the dataset
df.head()

## Analyse the continuous values by creating histograms to understand the distribution
df.hist(bins=50, figsize=(20,15))
plt.show()

"""#### Feature Engineering - Extracting Day, Month and Year from the column 'Data'"""

#Function to feature engineer the date attribute by separting the date value into date,month and year
def feature_engg(df):

    #As data is object type first changing its datatype to datetime
    df['Data']=pd.to_datetime(df['Data'],format='%m/%d/%Y %I:%M:%S %p')
    # Extract month, day, and year
    df['Month'] = df['Data'].dt.month
    df['Day'] = df['Data'].dt.day
    df['Year'] = df['Data'].dt.year
    return df

#Applying feature_engg function to train DF
feature_engg(df)
df.head()

"""### Missing Values"""

#checking for the missing values
df.isna().sum()

"""There are no missing values in our dataset"""

# Check for duplicates
df.duplicated()

"""### Outliers"""

#numerical columns
num_cols = df.select_dtypes(include='number')
def visualize_outliers(df):
    #visualizing the outliers using box ploat
    columns_to_plot = num_cols
    # Loop through each column and plot the box plot
    for i, col in enumerate(columns_to_plot):
      plt.figure(figsize=(10,5))
      plt.subplots_adjust(wspace=0.5, hspace=0.5)
      sns.boxplot(data=df[col])
      plt.title(f'Boxplot of {col}')
      plt.xlabel(num_cols)
      plt.ylabel('Value')
      print(plt.show())

#Applying function to train df
print("visualizing outliers before handling them", visualize_outliers(df))

# box plot of UNIXTIME
plt.figure(figsize=(10, 5))
sns.boxplot(x=df['UNIXTime'])
plt.show()



"""### Analyzing the relation between datetime variables and target feature 'Radiation'

"""

df_copy = df
date_cols = ['UNIXTime','Time','Year','Day','Month','TimeSunRise','TimeSunSet']
for col in date_cols:
  plt.figure(figsize=(10,5))
  df_copy.groupby(col)['Radiation'].median().plot()
  plt.xlabel(col)
  plt.ylabel('Radiation')
  plt.title("Radiation vs "+col)
  plt.show()

df['Year'].unique() #Only one value for the Year, we can drop it

"""We are going to drop UNIXTime, Time, Year and Day as there is no specific relation towards our target variable.
Also the column Data, as we already extracted the necessary information from it
"""

df_copy = df_copy.drop(columns=['UNIXTime', 'Time', 'Year','Day','Data'])

df_copy.info()

"""We will convert the columns TimeSunRise and TimeSunSet from object to datetime"""

df_copy.head()

#Function to convert TimeSunRise and TimeSunSet to int type
def time_to_int(time_str):
    h, m, _ = time_str.split(':')
    return int(h + m)

df_copy['TimeSunRise'] = df_copy['TimeSunRise'].apply(time_to_int)
df_copy['TimeSunSet'] = df_copy['TimeSunSet'].apply(time_to_int)





df_copy.columns





"""#### Stage 7: Data Analysis

#### *Target variable exploration*
"""

df_copy['Radiation'].describe()

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.hist(df_copy['Radiation'], bins=30, edgecolor='k', alpha=0.7)
plt.title('Histogram of Radiation ')
plt.xlabel(' Radiation')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()



"""Checking for the correlation"""

df_copy.corr()

plt.figure(figsize=(16, 6))
heatmap = sns.heatmap(df_copy.corr(), vmin=-1, vmax=1, annot=True,fmt=".2f", cmap='BrBG')
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12)
plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')

plt.figure(figsize=(5, 12))
heatmap = sns.heatmap(df_copy.corr()[['Radiation']].sort_values(by='Radiation', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Features Correlating with Radiation', fontdict={'fontsize':18}, pad=16);

df.columns

columns_predictors=['Temperature', 'Pressure',
       'Humidity', 'WindDirection(Degrees)', 'Speed', 'TimeSunRise',
       'TimeSunSet', 'Month']

#Spliting data into dependent and independent varaiables
X_train = df_copy[columns_predictors]
y_train = df_copy['Radiation']

print('X_train and y_train shape before one hot encoding and normilazation')
print(f"X_train.shape: {X_train.shape}")
print(f"y_train.shape: {y_train.shape}")

"""# Data normalization and Bulding Data Pipeline using Feature Union"""

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder


# Create a class to select numerical or categorical columns
# since Scikit-Learn doesn't handle DataFrames yet
class DataFrameSelector(BaseEstimator, TransformerMixin):
    def __init__(self, attribute_names):
        self.attribute_names = attribute_names
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[self.attribute_names].values

num_pipeline = Pipeline([
        ('selector', DataFrameSelector(columns_predictors)),
        ('std_scaler', StandardScaler()),
    ])

from sklearn.pipeline import FeatureUnion

full_pipeline = FeatureUnion(transformer_list=[
    ("num_pipeline", num_pipeline)

    ])

X_train_transform = full_pipeline.fit_transform(X_train)

# pipeline_full.fit(X_train)
# X_train_transform = pipeline_full.transform(X_train)
print("Shape after one hot encoding and normalisation")
print(f"X_train transformed.shape: {X_train_transform.shape}")

"""# ML model"""

# build a model

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

# Train a RandomForestRegressor model
forest_reg = RandomForestRegressor()
forest_reg.fit(X_train_transform, y_train)

# Evaluate the model using cross-validation
scores = cross_val_score(forest_reg, X_train_transform, y_train, scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)

print("RMSE scores:", rmse_scores)
print("Mean RMSE:", rmse_scores.mean())
print("Standard deviation of RMSE:", rmse_scores.std())